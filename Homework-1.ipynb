{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actor.gender' 'gr.amount' 'movie.country' 'movie.directed_by'\n",
      " 'movie.estimated_budget' 'movie.genre' 'movie.gross_revenue'\n",
      " 'movie.initial_release_date' 'movie.language' 'movie.locations'\n",
      " 'movie.music' 'movie.produced_by' 'movie.production_companies'\n",
      " 'movie.rating' 'movie.starring.actor' 'movie.starring.character'\n",
      " 'movie.subjects' 'person.date_of_birth']\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the data; you'll need to upload this file, or download the notebook and run it locally if you want this to work. \n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "data = pd.read_csv('./hw1_train.csv', index_col=0)\n",
    "data.columns = [\"text\", \"labels\"]\n",
    "                                                                                                                                                              \n",
    "data[\"labels\"].fillna(\"none\", inplace=True)\n",
    "data[\"labels\"] = data[\"labels\"].str.split()\n",
    "\n",
    "vocab_size = 1_000  # How many tokens to include in the vocabulary. Feel free to adjust this!\n",
    "        \n",
    "# create self.labels: a list of every possible label in the dataset\n",
    "labels = (\n",
    "    data\n",
    "    .drop(columns='text')\n",
    "    .explode('labels')\n",
    "    ['labels'].unique()\n",
    ").tolist()\n",
    "labels.remove('none')\n",
    "n_labels = len(labels)\n",
    "        \n",
    "# create self.label2id: a dictionary which maps the labels from self.labels (above) to a unique integer\n",
    "label2id = {v: k for k, v in dict(enumerate(labels)).items()}\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit([labels])\n",
    "print(mlb.classes_)\n",
    "\n",
    "df = data.copy()\n",
    "df[\"text\"] = df[\"text\"].str.split()\n",
    "## TODO REMOVE APOSTROPHE S MAYBE\n",
    "vocab = (\n",
    "    df\n",
    "    .drop(columns='labels')\n",
    "    .explode('text')\n",
    ")['text'].tolist()\n",
    "    \n",
    "vocab = Counter(vocab).most_common(vocab_size)\n",
    "for i in range(vocab_size):\n",
    "    y = list(vocab[i])\n",
    "    y[1] = i\n",
    "    vocab[i] = tuple(y)\n",
    "    # also, don't forget to include <unk> (unknown)\n",
    "    # TODO assign <unk> a unique ID. \n",
    "    # ======================================================================\n",
    "    \n",
    "vocab.append(('<unk>', vocab_size))\n",
    "vocab_size = vocab_size + 1 # plus 1 because <unk>\n",
    "\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 1156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create the transform\n",
    "kwargs = {\n",
    "    'ngram_range': (1,1),  # Use 1-grams + 2-grams.\n",
    "    'analyzer': 'word',  # Split text into word tokens.\n",
    "    'min_df': 1,\n",
    "    'stop_words': \"english\",\n",
    "}\n",
    "vectorizer = TfidfVectorizer(**kwargs)\n",
    "vectorizer.fit(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmath import isnan\n",
    "from collections import Counter\n",
    "import scipy\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, vocab):\n",
    "        self.data = df\n",
    "        self.vocab = vocab\n",
    "\n",
    "    \n",
    "    def one_hot_encode_labels(self, labels: List[str]):        \n",
    "        # TODO CHANGE TO MULTILABELBINALIZER?        \n",
    "        # encoded = np.zeros(n_labels)\n",
    "        # for element in labels:\n",
    "        #     if element in label2id:\n",
    "        #         encoded[label2id.get(element)] = 1\n",
    "        # encoded = encoded.astype(np.float32)\n",
    "        # return torch.from_numpy(encoded)\n",
    "        encoded = mlb.transform([labels])\n",
    "        encoded = encoded.astype(np.float32)\n",
    "        encoded = torch.squeeze(torch.from_numpy(encoded))\n",
    "        return encoded\n",
    "    \n",
    "    def tokenize(self, text: str):\n",
    "        return text.split()\n",
    "    \n",
    "    def encode_tokens(self, tokens: str):\n",
    "        vector = vectorizer.transform([tokens])\n",
    "        encoded = vector.toarray()\n",
    "        encoded = encoded.astype(np.float32)\n",
    "        encoded = torch.squeeze(torch.from_numpy(encoded))\n",
    "        return encoded\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, n: int):\n",
    "        input_to_model = self.data[\"text\"].iloc[n]\n",
    "        labels = self.data[\"labels\"].iloc[n]\n",
    "        return self.encode_tokens(input_to_model), self.one_hot_encode_labels(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "906\n"
     ]
    }
   ],
   "source": [
    "#TODO tuple output, fix\n",
    "\n",
    "train_set = MovieDataset(train_data, vocab)\n",
    "eval_set = MovieDataset(val_data, vocab)\n",
    "\n",
    "# A small batch size of 2 makes it easier to debug for printing. \n",
    "train_dataloader = DataLoader(train_set, batch_size=1)\n",
    "eval_dataloader = DataLoader(eval_set, batch_size=1)\n",
    "\n",
    "emsize = (len(train_dataloader.dataset[0][0]))\n",
    "print(emsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 906]) torch.Size([1, 18])\n",
      "torch.Size([1, 906]) torch.Size([1, 18])\n",
      "torch.Size([1, 906]) torch.Size([1, 18])\n",
      "torch.Size([1, 906]) torch.Size([1, 18])\n",
      "torch.Size([1, 906]) torch.Size([1, 18])\n"
     ]
    }
   ],
   "source": [
    "# Zipping the dataloader with range(N) lets us only print the first N batches\n",
    "for _, batch in zip(range(5), train_dataloader):\n",
    "    # Do something here; maybe print the batch to see if it looks right to you?\n",
    "    print(batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# inputs  = 1016\n",
    "# outputs = 18\n",
    " \n",
    "# net = nn.Sequential(\n",
    "#       nn.Linear(inputs, 508),\n",
    "#       nn.ReLU(),\n",
    "#       nn.Linear(508, outputs),\n",
    "#       nn.Sigmoid()\n",
    "#       ).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the model\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class MyModel(nn.Module):\n",
    "\n",
    "#     def __init__(self, embed_dim, hidden_dim, output_dim):\n",
    "#         super().__init__()\n",
    "#         #self.embedding = nn.EmbeddingBag(n_tokens, embed_dim, sparse=True)\n",
    "#         # self.init_weights()\n",
    "#         self.relu1 = nn.ReLU()\n",
    "#         self.relu2 = nn.ReLU()\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         self.fc1 = nn.Linear(int(embed_dim), int(hidden_dim))\n",
    "#         self.fc2 = nn.Linear(int(hidden_dim), int(hidden_dim/2))\n",
    "#         self.fc3 = nn.Linear(int(hidden_dim/2), int(output_dim))\n",
    "\n",
    "\n",
    "#     # def init_weights(self):\n",
    "#     #     initrange = 0.5\n",
    "#     #     self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "#     #     self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "#     #     self.fc.bias.data.zero_()\n",
    "\n",
    "#     def forward(self, text):\n",
    "#         # embedded = self.embedding(text.long())\n",
    "#         hidden1 = self.relu1(self.fc1(text))\n",
    "#         hidden2 = self.relu2(self.fc2(hidden1))\n",
    "#         hidden3 = self.relu3(self.fc3(hidden2))\n",
    "#         return hidden3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "  \n",
    " \n",
    "   def __init__(self, input_size, hidden_sizes, output_size, dropout=False, dropout_p=0.1):\n",
    "       super(MyModel, self).__init__()\n",
    "       self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "       self.fc2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])\n",
    "       self.fc3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])\n",
    "       self.fc4 = nn.Linear(hidden_sizes[2], output_size)\n",
    "       # self.fc5 = nn.Sigmoid()\n",
    " \n",
    "       self.add_dropout = dropout\n",
    "       self.dropout = nn.Dropout(dropout_p)\n",
    " \n",
    "   def forward(self, x):\n",
    "       h1 = F.relu(self.fc1(x))\n",
    "       h2 = F.relu(self.fc2(h1))\n",
    "       h3 = F.relu(self.fc3(h2))\n",
    "       # h4 = F.relu(self.fc4(h3))\n",
    "       if self.add_dropout:\n",
    "           logits = self.fc4(self.dropout(h3))\n",
    "       else:\n",
    "           logits = self.fc4(h3)\n",
    "       return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "n_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "hsize = emsize/2\n",
    "hidden_sizes = [500, 200, 64]\n",
    "output_size = n_labels\n",
    "\n",
    "model = MyModel(emsize, hidden_sizes, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import logit\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def train(loader, model, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    pbar = tqdm(loader)\n",
    "    for x, y in pbar:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = loss_fn(logits, y)\n",
    "        pbar.set_postfix({'loss': loss.item()})\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return sum(losses) / len(losses)\n",
    "\n",
    "\n",
    "def evaluate(loader, model, loss_fn, score_fn):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    lbs = []\n",
    "    for x, y in tqdm(loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        # loss = loss_fn(logits, y)\n",
    "\n",
    "        pred = torch.sigmoid(logits)\n",
    "        pred = pred.detach()\n",
    "        pred = np.round(np.array(pred.cpu()))\n",
    "\n",
    "        predictions.append(pred)\n",
    "        lbs.append(y.cpu().numpy())\n",
    "    \n",
    "    predictions =  np.array(predictions)\n",
    "    lbs = np.array(lbs)\n",
    "    predictions = np.reshape(predictions, (predictions.shape[0], 18))\n",
    "    lbs = np.reshape(lbs, (lbs.shape[0], 18))\n",
    "\n",
    "\n",
    "    score = score_fn(lbs, predictions, target_names=labels)\n",
    "    f1score = f1_score(lbs, predictions, average=\"weighted\")\n",
    "    return f1score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "score_fn = classification_report\n",
    "best_acc = 0\n",
    "# with open(\"output.txt\", \"a\") as f:\n",
    "#     for epoch in range(n_epochs):\n",
    "#         avg_loss = train(train_dataloader, model, optimizer, loss_fn)\n",
    "#         accuracy = evaluate(eval_dataloader, model, loss_fn, score_fn)\n",
    "#         print('train loss: ', avg_loss, file=f)\n",
    "#         print('val accuracy: \\n ', accuracy, file=f)\n",
    "#     # if accuracy > best_acc and accuracy > 0.7:\n",
    "#     #     torch.save(model.state_dict(), f'best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1849/1849 [00:14<00:00, 130.64it/s, loss=0.189]\n",
      "100%|██████████| 463/463 [00:00<00:00, 845.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16277670935733826\n",
      "0.512199154040608\n",
      "AVG Training Loss:0.163 % AVG Test Acc 0.51 %\n",
      "---------------------------------------------\n",
      "\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1849/1849 [00:13<00:00, 133.81it/s, loss=0.0509]\n",
      "100%|██████████| 463/463 [00:00<00:00, 820.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0660432435903973\n",
      "0.7656241268764131\n",
      "AVG Training Loss:0.066 % AVG Test Acc 0.77 %\n",
      "---------------------------------------------\n",
      "\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1849/1849 [00:13<00:00, 132.81it/s, loss=0.0114]\n",
      "100%|██████████| 463/463 [00:00<00:00, 795.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03410463216463805\n",
      "0.8250126225072106\n",
      "AVG Training Loss:0.034 % AVG Test Acc 0.83 %\n",
      "---------------------------------------------\n",
      "\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1849/1849 [00:14<00:00, 129.17it/s, loss=0.00257]\n",
      "100%|██████████| 463/463 [00:00<00:00, 755.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.022306961537515966\n",
      "0.8285859079078989\n",
      "AVG Training Loss:0.022 % AVG Test Acc 0.83 %\n",
      "---------------------------------------------\n",
      "\n",
      "Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1849/1849 [00:16<00:00, 109.40it/s, loss=0.000283]\n",
      "100%|██████████| 463/463 [00:00<00:00, 694.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016551151142978393\n",
      "0.8290721642105949\n",
      "AVG Training Loss:0.017 % AVG Test Acc 0.83 %\n",
      "---------------------------------------------\n",
      "\n",
      "Fold 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1849/1849 [00:17<00:00, 103.18it/s, loss=0.000281]\n",
      "100%|██████████| 463/463 [00:00<00:00, 598.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01386172805764017\n",
      "0.8321650066627552\n",
      "AVG Training Loss:0.014 % AVG Test Acc 0.83 %\n",
      "---------------------------------------------\n",
      "\n",
      "Fold 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1849/1849 [00:15<00:00, 119.80it/s, loss=0.000495]\n",
      "100%|██████████| 463/463 [00:00<00:00, 827.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011666307279169367\n",
      "0.8316727543219065\n",
      "AVG Training Loss:0.012 % AVG Test Acc 0.83 %\n",
      "---------------------------------------------\n",
      "\n",
      "Fold 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1849/1849 [00:13<00:00, 133.91it/s, loss=0.000682]\n",
      "100%|██████████| 463/463 [00:00<00:00, 800.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010227203276237835\n",
      "0.8255788562517296\n",
      "AVG Training Loss:0.010 % AVG Test Acc 0.83 %\n",
      "---------------------------------------------\n",
      "\n",
      "Fold 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1849/1849 [00:13<00:00, 134.00it/s, loss=0.000282]\n",
      "100%|██████████| 463/463 [00:00<00:00, 821.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008200756416875817\n",
      "0.8303300495329\n",
      "AVG Training Loss:0.008 % AVG Test Acc 0.83 %\n",
      "---------------------------------------------\n",
      "\n",
      "Fold 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1849/1849 [00:13<00:00, 134.25it/s, loss=6.48e-5]\n",
      "100%|██████████| 463/463 [00:00<00:00, 837.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007404133098130679\n",
      "0.8275412783701537\n",
      "AVG Training Loss:0.007 % AVG Test Acc 0.83 %\n",
      "---------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy import split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "foldperf = {}\n",
    "k = 10\n",
    "splits = KFold(n_splits = k, shuffle = True, random_state = 42)\n",
    "for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(data)))):\n",
    "\n",
    "    print('Fold {}'.format(fold + 1))\n",
    "    history = {'train_loss': [], 'eval_score':[]}\n",
    "\n",
    "    for epoch in range(1):\n",
    "        train_loss = train(train_dataloader, model, optimizer, loss_fn)\n",
    "        eval_score = evaluate(eval_dataloader, model, loss_fn, score_fn)\n",
    "        print(train_loss)\n",
    "        print(eval_score)\n",
    "\n",
    "        print(\"AVG Training Loss:{:.3f} % AVG Test Acc {:.2f} %\".format(train_loss, eval_score))\n",
    "        print(\"---------------------------------------------\\n\")\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['eval_score'].append(eval_score)\n",
    "\n",
    "    foldperf['fold{}'.format(fold+1)] = history  \n",
    "\n",
    "torch.save(model,'myModel.pt')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('myModel.pt')\n",
    "\n",
    "test_data = pd.read_csv('./hw1_test.csv')\n",
    "\n",
    "x_submission = np.array(test_data['UTTERANCES'])\n",
    "x_submission_transformed = vectorizer.transform(x_submission).toarray()\n",
    "x_submission_transformed = x_submission_transformed.astype(np.float32)\n",
    "x_submission_transformed_ts = torch.Tensor(x_submission_transformed)\n",
    "\n",
    "def prediction_func(x_submission, model):\n",
    "    predictions = []\n",
    "    for i , x in enumerate(x_submission):\n",
    "        logits = model(x)\n",
    "        pred = torch.sigmoid(logits)\n",
    "        pred = pred.detach()\n",
    "        pred = np.round(np.array(pred.cpu()))\n",
    "        predictions.append(pred)\n",
    "    predictions = np.array(predictions)\n",
    "    pred_classes = mlb.inverse_transform(predictions)\n",
    " \n",
    "    reformat_pred = []\n",
    "    for pred in pred_classes:\n",
    "       pred_lst = []\n",
    "       for i in range(len(pred)):\n",
    "         pred_lst.append(pred[i])\n",
    "       reformat_pred.append(np.array(pred_lst))\n",
    "    return np.array(reformat_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = prediction_func(x_submission_transformed_ts, model)\n",
    "\n",
    "test_data['CORE RELATIONS'] = preds\n",
    "\n",
    "test_data['CORE RELATIONS'] = test_data['CORE RELATIONS'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "submission_data = test_data[['ID', 'CORE RELATIONS']]\n",
    "\n",
    "submission_data.to_csv('submission.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3437ece83d9be1a611a10f7642590578caefda86989dbeb196251210a92674a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
